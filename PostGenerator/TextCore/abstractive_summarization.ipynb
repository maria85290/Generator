{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstractive summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5 transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def T5_summarization_model (text):\n",
    "    # initialize the model architecture and weights\n",
    "    t5model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "    # initialize the model tokenizer\n",
    "    t5tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "    t5tokenized_text = t5tokenizer.encode(\"summarize:\"+ text,\n",
    "                                    truncation=True,\n",
    "                                    return_attention_mask=True,\n",
    "                                    add_special_tokens=True, \n",
    "                                    padding='max_length',     \n",
    "                                    return_tensors=\"pt\").to(device)\n",
    "\n",
    "\n",
    "    t5summary_ids =  t5model.generate(input_ids=t5tokenized_text,\n",
    "                    num_beams=3,  ##modelo olha para 3 possiveis palavras\n",
    "                    min_length=20, ## number min of tokens\n",
    "                    max_length=70,  ##number maximo of tokens\n",
    "                    repetition_penalty=1.0,\n",
    "                    early_stopping=True)\n",
    "\n",
    "    output = t5tokenizer.decode(t5summary_ids[0],  \n",
    "                             skip_special_tokens=True, \n",
    "                             clean_up_tokenization_spaces=True)\n",
    "    return  output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization with BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from summarizer import Summarizer\n",
    "from summarizer import TransformerSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def bert_summarization_model (text):\n",
    "    #Create default summarizer model\n",
    "    # By default bert-extractive-summarizer uses the ‘bert-large-uncased‘ pretrained model.\n",
    "    bert_model = Summarizer()\n",
    "\n",
    "    summary = ''.join(bert_model(text, max_length=50))\n",
    "    return summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 571/571 [00:00<00:00, 244kB/s]\n",
      "Downloading: 100%|██████████| 1.34G/1.34G [01:54<00:00, 11.8MB/s] \n",
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Downloading: 100%|██████████| 232k/232k [00:00<00:00, 712kB/s] \n",
      "Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 8.31kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And he called her his “very gentle Valentine.” She first advertised her business in 1852.”\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azrieli Center, 26 Harokmim St., Holon, Israel.”\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m         data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      4\u001b[0m         f\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m----> 6\u001b[0m summary \u001b[38;5;241m=\u001b[39m bert_summarization_model (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpostText\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(summary)\n",
      "Cell \u001b[0;32mIn [4], line 4\u001b[0m, in \u001b[0;36mbert_summarization_model\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbert_summarization_model\u001b[39m (text):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m#Create default summarizer model\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# By default bert-extractive-summarizer uses the ‘bert-large-uncased‘ pretrained model.\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     bert_model \u001b[38;5;241m=\u001b[39m \u001b[43mSummarizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(bert_model(text, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m))\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m summary\n",
      "File \u001b[0;32m~/miniforge3/envs/py3/lib/python3.9/site-packages/summarizer/bert.py:81\u001b[0m, in \u001b[0;36mSummarizer.__init__\u001b[0;34m(self, model, custom_model, custom_tokenizer, hidden, reduce_option, sentence_handler, random_state, hidden_concat, gpu_id)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/summarizer/bert.py?line=55'>56</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m     <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/summarizer/bert.py?line=56'>57</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/summarizer/bert.py?line=57'>58</a>\u001b[0m     model: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbert-large-uncased\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/summarizer/bert.py?line=65'>66</a>\u001b[0m     gpu_id: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/summarizer/bert.py?line=66'>67</a>\u001b[0m ):\n\u001b[1;32m     <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/summarizer/bert.py?line=67'>68</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/summarizer/bert.py?line=68'>69</a>\u001b[0m \u001b[39m    This is the main Bert Summarizer class.\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/summarizer/bert.py?line=69'>70</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/summarizer/bert.py?line=77'>78</a>\u001b[0m \u001b[39m    :param gpu_id: GPU device index if CUDA is available. \u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/summarizer/bert.py?line=78'>79</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/summarizer/bert.py?line=80'>81</a>\u001b[0m     \u001b[39msuper\u001b[39;49m(Summarizer, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m     <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/summarizer/bert.py?line=81'>82</a>\u001b[0m         model, custom_model, custom_tokenizer, hidden, reduce_option, sentence_handler, random_state, hidden_concat,\n\u001b[1;32m     <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/summarizer/bert.py?line=82'>83</a>\u001b[0m         gpu_id\n\u001b[1;32m     <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/summarizer/bert.py?line=83'>84</a>\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/py3/lib/python3.9/site-packages/summarizer/bert.py:49\u001b[0m, in \u001b[0;36mBertSummarizer.__init__\u001b[0;34m(self, model, custom_model, custom_tokenizer, hidden, reduce_option, sentence_handler, random_state, hidden_concat, gpu_id)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/summarizer/bert.py?line=22'>23</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m     <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/summarizer/bert.py?line=23'>24</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/summarizer/bert.py?line=24'>25</a>\u001b[0m     model: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbert-large-uncased\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/summarizer/bert.py?line=32'>33</a>\u001b[0m     gpu_id: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/summarizer/bert.py?line=33'>34</a>\u001b[0m ):\n\u001b[1;32m     <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/summarizer/bert.py?line=34'>35</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/summarizer/bert.py?line=35'>36</a>\u001b[0m \u001b[39m    This is the parent Bert Summarizer model. New methods should implement this class.\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/summarizer/bert.py?line=36'>37</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/summarizer/bert.py?line=46'>47</a>\u001b[0m \u001b[39m    :param gpu_id: GPU device index if CUDA is available.\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/summarizer/bert.py?line=47'>48</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/summarizer/bert.py?line=48'>49</a>\u001b[0m     model \u001b[39m=\u001b[39m BertEmbedding(model, custom_model, custom_tokenizer, gpu_id)\n\u001b[1;32m     <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/summarizer/bert.py?line=49'>50</a>\u001b[0m     model_func \u001b[39m=\u001b[39m partial(model, hidden\u001b[39m=\u001b[39mhidden, reduce_option\u001b[39m=\u001b[39mreduce_option, hidden_concat\u001b[39m=\u001b[39mhidden_concat)\n\u001b[1;32m     <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/summarizer/bert.py?line=50'>51</a>\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(model_func, sentence_handler, random_state)\n",
      "File \u001b[0;32m~/miniforge3/envs/py3/lib/python3.9/site-packages/summarizer/transformer_embeddings/bert_embedding.py:52\u001b[0m, in \u001b[0;36mBertEmbedding.__init__\u001b[0;34m(self, model, custom_model, custom_tokenizer, gpu_id)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/summarizer/transformer_embeddings/bert_embedding.py?line=49'>50</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m custom_model\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/summarizer/transformer_embeddings/bert_embedding.py?line=50'>51</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/summarizer/transformer_embeddings/bert_embedding.py?line=51'>52</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m base_model\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m     <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/summarizer/transformer_embeddings/bert_embedding.py?line=52'>53</a>\u001b[0m         model, output_hidden_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/summarizer/transformer_embeddings/bert_embedding.py?line=54'>55</a>\u001b[0m \u001b[39mif\u001b[39;00m custom_tokenizer:\n\u001b[1;32m     <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/summarizer/transformer_embeddings/bert_embedding.py?line=55'>56</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m custom_tokenizer\n",
      "File \u001b[0;32m~/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py:2241\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=2237'>2238</a>\u001b[0m     \u001b[39mif\u001b[39;00m dtype_orig \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=2238'>2239</a>\u001b[0m         torch\u001b[39m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[0;32m-> <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=2240'>2241</a>\u001b[0m     model, missing_keys, unexpected_keys, mismatched_keys, error_msgs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_load_pretrained_model(\n\u001b[1;32m   <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=2241'>2242</a>\u001b[0m         model,\n\u001b[1;32m   <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=2242'>2243</a>\u001b[0m         state_dict,\n\u001b[1;32m   <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=2243'>2244</a>\u001b[0m         loaded_state_dict_keys,  \u001b[39m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=2244'>2245</a>\u001b[0m         resolved_archive_file,\n\u001b[1;32m   <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=2245'>2246</a>\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=2246'>2247</a>\u001b[0m         ignore_mismatched_sizes\u001b[39m=\u001b[39;49mignore_mismatched_sizes,\n\u001b[1;32m   <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=2247'>2248</a>\u001b[0m         sharded_metadata\u001b[39m=\u001b[39;49msharded_metadata,\n\u001b[1;32m   <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=2248'>2249</a>\u001b[0m         _fast_init\u001b[39m=\u001b[39;49m_fast_init,\n\u001b[1;32m   <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=2249'>2250</a>\u001b[0m         low_cpu_mem_usage\u001b[39m=\u001b[39;49mlow_cpu_mem_usage,\n\u001b[1;32m   <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=2250'>2251</a>\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=2251'>2252</a>\u001b[0m         offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=2252'>2253</a>\u001b[0m         offload_state_dict\u001b[39m=\u001b[39;49moffload_state_dict,\n\u001b[1;32m   <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=2253'>2254</a>\u001b[0m         dtype\u001b[39m=\u001b[39;49mtorch_dtype,\n\u001b[1;32m   <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=2254'>2255</a>\u001b[0m         load_in_8bit\u001b[39m=\u001b[39;49mload_in_8bit,\n\u001b[1;32m   <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=2255'>2256</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=2257'>2258</a>\u001b[0m \u001b[39m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=2258'>2259</a>\u001b[0m model\u001b[39m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py:2432\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, load_in_8bit)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=2421'>2422</a>\u001b[0m \u001b[39mif\u001b[39;00m state_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=2422'>2423</a>\u001b[0m     \u001b[39m# Whole checkpoint\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=2423'>2424</a>\u001b[0m     mismatched_keys \u001b[39m=\u001b[39m _find_mismatched_keys(\n\u001b[1;32m   <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=2424'>2425</a>\u001b[0m         state_dict,\n\u001b[1;32m   <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=2425'>2426</a>\u001b[0m         model_state_dict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=2429'>2430</a>\u001b[0m         ignore_mismatched_sizes,\n\u001b[1;32m   <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=2430'>2431</a>\u001b[0m     )\n\u001b[0;32m-> <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=2431'>2432</a>\u001b[0m     error_msgs \u001b[39m=\u001b[39m _load_state_dict_into_model(model_to_load, state_dict, start_prefix)\n\u001b[1;32m   <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=2432'>2433</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=2433'>2434</a>\u001b[0m     \u001b[39m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=2434'>2435</a>\u001b[0m \n\u001b[1;32m   <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=2435'>2436</a>\u001b[0m     \u001b[39m# This should always be a list but, just to be sure.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=2436'>2437</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(resolved_archive_file, \u001b[39mlist\u001b[39m):\n",
      "File \u001b[0;32m~/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py:447\u001b[0m, in \u001b[0;36m_load_state_dict_into_model\u001b[0;34m(model_to_load, state_dict, start_prefix)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=443'>444</a>\u001b[0m         \u001b[39mif\u001b[39;00m child \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=444'>445</a>\u001b[0m             load(child, state_dict, prefix \u001b[39m+\u001b[39m name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=446'>447</a>\u001b[0m load(model_to_load, state_dict, prefix\u001b[39m=\u001b[39;49mstart_prefix)\n\u001b[1;32m    <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=447'>448</a>\u001b[0m \u001b[39m# Delete `state_dict` so it could be collected by GC earlier. Note that `state_dict` is a copy of the argument, so\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=448'>449</a>\u001b[0m \u001b[39m# it's safe to delete it.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=449'>450</a>\u001b[0m \u001b[39mdel\u001b[39;00m state_dict\n",
      "File \u001b[0;32m~/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py:445\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=442'>443</a>\u001b[0m \u001b[39mfor\u001b[39;00m name, child \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39m_modules\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=443'>444</a>\u001b[0m     \u001b[39mif\u001b[39;00m child \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=444'>445</a>\u001b[0m         load(child, state_dict, prefix \u001b[39m+\u001b[39;49m name \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py:445\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=442'>443</a>\u001b[0m \u001b[39mfor\u001b[39;00m name, child \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39m_modules\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=443'>444</a>\u001b[0m     \u001b[39mif\u001b[39;00m child \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=444'>445</a>\u001b[0m         load(child, state_dict, prefix \u001b[39m+\u001b[39;49m name \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "    \u001b[0;31m[... skipping similar frames: _load_state_dict_into_model.<locals>.load at line 445 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py:445\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=442'>443</a>\u001b[0m \u001b[39mfor\u001b[39;00m name, child \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39m_modules\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=443'>444</a>\u001b[0m     \u001b[39mif\u001b[39;00m child \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=444'>445</a>\u001b[0m         load(child, state_dict, prefix \u001b[39m+\u001b[39;49m name \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py:441\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=438'>439</a>\u001b[0m                     module\u001b[39m.\u001b[39m_load_from_state_dict(\u001b[39m*\u001b[39margs)\n\u001b[1;32m    <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=439'>440</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=440'>441</a>\u001b[0m         module\u001b[39m.\u001b[39;49m_load_from_state_dict(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=442'>443</a>\u001b[0m \u001b[39mfor\u001b[39;00m name, child \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39m_modules\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/transformers/modeling_utils.py?line=443'>444</a>\u001b[0m     \u001b[39mif\u001b[39;00m child \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/py3/lib/python3.9/site-packages/torch/nn/modules/module.py:1507\u001b[0m, in \u001b[0;36mModule._load_from_state_dict\u001b[0;34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1504'>1505</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1505'>1506</a>\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m-> <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1506'>1507</a>\u001b[0m         param\u001b[39m.\u001b[39;49mcopy_(input_param)\n\u001b[1;32m   <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1507'>1508</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m ex:\n\u001b[1;32m   <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1508'>1509</a>\u001b[0m     error_msgs\u001b[39m.\u001b[39mappend(\u001b[39m'\u001b[39m\u001b[39mWhile copying the parameter named \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1509'>1510</a>\u001b[0m                       \u001b[39m'\u001b[39m\u001b[39mwhose dimensions in the model are \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1510'>1511</a>\u001b[0m                       \u001b[39m'\u001b[39m\u001b[39mwhose dimensions in the checkpoint are \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1511'>1512</a>\u001b[0m                       \u001b[39m'\u001b[39m\u001b[39man exception occurred : \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   <a href='file:///Users/mariabarbosa/miniforge3/envs/py3/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1512'>1513</a>\u001b[0m                       \u001b[39m.\u001b[39mformat(key, param\u001b[39m.\u001b[39msize(), input_param\u001b[39m.\u001b[39msize(), ex\u001b[39m.\u001b[39margs))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for file in os.listdir(\"../../../extractors/Snopes/extractions\")[:5]:\n",
    "    with open(\"../../../extractors/Snopes/extractions/\" + file) as f:\n",
    "            data = json.load(f)\n",
    "            f.close()\n",
    "\n",
    "    summary = bert_summarization_model (' '.join(data['postText']))\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization with GPT2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summarizer import TransformerSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gtp2_summariation_model(text):\n",
    "    GPT2_model = TransformerSummarizer(transformer_type=\"GPT2\",\n",
    "                         transformer_model_key=\"gpt2-medium\")\n",
    "    summary = ''.join(GPT2_model(text,  max_length=50))\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(\"../../../extractors/Snopes/extractions\")[:1]:\n",
    "    with open(\"../../../extractors/Snopes/extractions/\" + file) as f:\n",
    "            data = json.load(f)\n",
    "            f.close()\n",
    "\n",
    "    summary = gtp2_summariation_model (' '.join(data['postText']))\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization with XLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XLNet_summarization_model (text):\n",
    "    xlnet_model = TransformerSummarizer(transformer_type=\"XLNet\",\n",
    "                     transformer_model_key=\"xlnet-base-cased\")\n",
    "    summary = ''.join(xlnet_model(text, max_length=50))\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetModel: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Downloading: 100%|██████████| 798k/798k [00:00<00:00, 1.73MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azrieli Center, 26 Harokmim St., Holon, Israel.”\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(\"../../../extractors/Snopes/extractions\")[2:3]:\n",
    "    with open(\"../../../extractors/Snopes/extractions/\" + file) as f:\n",
    "            data = json.load(f)\n",
    "            f.close()\n",
    "\n",
    "    summary = XLNet_summarization_model (' '.join(data['postText']))\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summaziton with BART MODEL \n",
    "\n",
    "#### From META"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bart_summarization_model(text):\n",
    "    summarizer = pipeline('summarization', model='facebook/bart-base', tokenizer='facebook/bart-large-cnn')\n",
    "   \n",
    "    summary = summarizer(text, max_length = 50)[0]['summary_text']\n",
    "    return summary\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose 100 posts for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "for file in os.listdir(\"../../../extractors/Snopes/extractions\")[:100]:\n",
    "    with open(\"../../../extractors/Snopes/extractions/\" + file) as f:\n",
    "            data = json.load(f)\n",
    "            f.close()\n",
    "\n",
    "\n",
    "\n",
    "    summary_T5 = T5_summarization_model (' '.join(data['postText']))\n",
    "    summary_BERT = bert_summarization_model(' '.join(data['postText']))\n",
    "    summary_BART = bart_summarization_model(' '.join(data['postText']))\n",
    "    summary_XLNET =  XLNet_summarization_model(' '.join(data['postText']))\n",
    "    summary_GTP2 = gtp2_summariation_model (' '.join(data['postText']))\n",
    "    dict = {'id': data['id'], 'allegation':data['allegation'], 'evaluation':data['evaluation'], 'T5':summary_T5, 'BERT': summary_BERT, 'BART': summary_BART, 'XLNet':summary_XLNET , 'GTP2': summary_GTP2 }\n",
    "\n",
    "    df = df.append(dict, ignore_index = True)\n",
    "\n",
    "\n",
    "df.to_csv('abs_summarization.csv')\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
