{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractive summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/py3/lib/python3.9/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.0.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "# Load Packages\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sumy library\n",
    "def text_rank (text):\n",
    "    parser = PlaintextParser.from_string(text,Tokenizer(\"english\"))\n",
    "\n",
    "    # Summarize using sumy TextRank\n",
    "    summarizer = TextRankSummarizer()\n",
    "    summary =summarizer(parser.document,1)\n",
    "    text_summary=\"\"\n",
    "\n",
    "    for sentence in summary:\n",
    "        text_summary +=str(sentence)\n",
    "\n",
    "    return (text_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## using NLTK\n",
    "def nltk_extractive_sumy(text):\n",
    "\n",
    "    stopWords = set(stopwords.words(\"english\"))\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    freqTable = dict()  ## Frequencia das palavras\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if word in stopWords:\n",
    "            continue\n",
    "        if word in freqTable:   \n",
    "            freqTable[word] += 1\n",
    "        else:\n",
    "            freqTable[word] = 1\n",
    "\n",
    "\n",
    "\n",
    "    sentences = sent_tokenize(text)  \n",
    "    sentenceValue = dict() ## Importancia das frases\n",
    "\n",
    "    for sentence in sentences:\n",
    "        for word, freq in freqTable.items():\n",
    "            if word in sentence.lower():\n",
    "              if sentence in sentenceValue:\n",
    "                sentenceValue[sentence] += freq\n",
    "              else:\n",
    "                sentenceValue[sentence] = freq\n",
    "\n",
    "    summary = ''\n",
    "\n",
    "    sentenceValue = dict(sorted(sentenceValue.items(), key=lambda item: item[1], reverse=True))\n",
    "   \n",
    "    for sentence in sentenceValue:\n",
    "\n",
    "        if len(summary)<230:\n",
    "        #and (sentenceValue[sentence] > (1.40 * average)):\n",
    "            summary += \" \" + sentence\n",
    "\n",
    "    return (summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the summarizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "\n",
    "# Parsing the text string using PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "\n",
    "\n",
    "def LSA_extractive_sumy (original_text):\n",
    "    parser=PlaintextParser.from_string(original_text,Tokenizer('english'))\n",
    "    lista = []\n",
    "    # creating the summarizer\n",
    "    lsa_summarizer=LsaSummarizer()\n",
    "    lsa_summary= lsa_summarizer(parser.document,1)\n",
    "    for sentence in lsa_summary:\n",
    "        lista.append(sentence)\n",
    "\n",
    "    return lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the summarizer\n",
    "from sumy.summarizers.luhn import LuhnSummarizer\n",
    "\n",
    "\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "\n",
    "def LuhnSum (original_text):\n",
    "    parser=PlaintextParser.from_string(original_text,Tokenizer('english'))\n",
    "\n",
    "    #  Creating the summarizer\n",
    "    luhn_summarizer=LuhnSummarizer()\n",
    "    luhn_summary=luhn_summarizer(parser.document,sentences_count=1)\n",
    "\n",
    "    # Printing the summary\n",
    "    for sentence in luhn_summary:\n",
    "        print(sentence)\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That was because while something depicted in the grainy, black-and-white version of the photograph taken by a camera aboard the rover could be interpreted to resemble the shape of a door, the agency’s Jet Propulsion Laboratory (JPL) told Snopes that it actually was a “very, very, very zoomed in shot of a tiny crevice in a rock.” “The team’s scientists underlined just how small [the crevice] is: roughly 30 centimeters wide and 45 centimeters across (11 by 17 inches),” a JPL spokesperson said via email.\n",
      "[<Sentence: “They said there are linear fractures throughout this outcrop, and this is a location where several linear fractures happen to intersect.” The photograph was taken by the Mast Camera (Mastcam) outfitted aboard Curiosity, a system that uses fixed-focal length, multispectral imagers to capture “true color” images of the red planet and beyond.>]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\"../../../extractors/Snopes/example_1.json\") as f:\n",
    "            d = json.load(f)\n",
    "            f.close()\n",
    "text = ' '.join(d['postText'])\n",
    "\n",
    "textRank_sumy = text_rank(text) \n",
    "textRank_nltk = nltk_extractive_sumy(text)\n",
    "print(textRank_sumy)\n",
    "print (textRank_nltk)\n",
    "\n",
    "\n",
    "lsa = LSA_extractive_sumy (text)\n",
    "luhn = LuhnSum(text)\n",
    "\n",
    "print(lsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textRank_sumy = text_rank(text) \n",
    "textRank_nltk = nltk_extractive_sumy(text)\n",
    "print(textRank_sumy)\n",
    "print (textRank_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guardar no dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "\n",
    "data = pd.DataFrame()\n",
    "\n",
    "for file in os.listdir(\"../../../extractors/Snopes/extractions\")[:100]:\n",
    "    with open(\"../../../extractors/Snopes/extractions/\" + file) as f:\n",
    "            d = json.load(f)\n",
    "            f.close()\n",
    "    text = ' '.join(d['postText'])\n",
    "\n",
    "    dic = {'id': d['id'], 'textRank_sumy': text_rank(text) , 'textRank_nltk': nltk_extractive_sumy(text) }\n",
    "\n",
    "    data = data.append(dic, ignore_index = True)\n",
    "\n",
    "    data.to_csv('ext_summarization.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_abs = pd.read_csv('abs_summarization.csv', index_col=0)\n",
    "df_abs.head()\n",
    "\n",
    "df_ext = pd.read_csv('ext_summarization.csv', index_col=0)\n",
    "df_ext.head()\n",
    "\n",
    "\n",
    "for ((i_a, r_a),(i_e, r_e))  in zip(df_abs.iterrows(),df_ext.iterrows()):\n",
    "        if (r_a[0] == r_e[0]): ## tem o mesmo id\n",
    "                df_abs.loc[i_a,'textRank_sumy'] = r_e[1]\n",
    "                df_abs.loc[i_a,'textRank_nltk'] = r_e[2]\n",
    "\n",
    "\n",
    "df_abs.to_csv('summarization.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>allegation</th>\n",
       "      <th>evaluation</th>\n",
       "      <th>T5</th>\n",
       "      <th>BERT</th>\n",
       "      <th>BART</th>\n",
       "      <th>XLNet</th>\n",
       "      <th>GTP2</th>\n",
       "      <th>textRank_sumy</th>\n",
       "      <th>textRank_nltk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.650725e+09</td>\n",
       "      <td>Valentine's Day was invented by greeting card ...</td>\n",
       "      <td>false</td>\n",
       "      <td>the holiday falls yearly on february 14. some ...</td>\n",
       "      <td>And he called her his “very gentle Valentine.”...</td>\n",
       "      <td>The idea that Valentine’s Day, which falls yea...</td>\n",
       "      <td>And he called her his “very gentle Valentine.”...</td>\n",
       "      <td>And he called her his “very gentle Valentine.”...</td>\n",
       "      <td>Although one can’t factually argue the holiday...</td>\n",
       "      <td>Although one can’t factually argue the holida...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.650726e+09</td>\n",
       "      <td>A video shows truckers from South Carolina on ...</td>\n",
       "      <td>miscaptioned</td>\n",
       "      <td>video shows truckers from south carolina in th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In January 2022, as hundreds of truckers drove...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In January 2022, as hundreds of truckers drove...</td>\n",
       "      <td>In January 2022, as hundreds of truckers drov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.650725e+09</td>\n",
       "      <td>An online advertisement revealed an unusual or...</td>\n",
       "      <td>false</td>\n",
       "      <td>for at least several years, online advertiseme...</td>\n",
       "      <td>Azrieli Center, 26 Harokmim St., Holon, Israel.”</td>\n",
       "      <td>For at least several years, online advertiseme...</td>\n",
       "      <td>Azrieli Center, 26 Harokmim St., Holon, Israel.”</td>\n",
       "      <td>Azrieli Center, 26 Harokmim St., Holon, Israel.”</td>\n",
       "      <td>A spokesperson for Farmers Insurance responded...</td>\n",
       "      <td>A spokesperson for Farmers Insurance responde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.650725e+09</td>\n",
       "      <td>Russia bombed a Biden-owned villa and several ...</td>\n",
       "      <td>false</td>\n",
       "      <td>a fictitious article falsely claimed a villa o...</td>\n",
       "      <td>Real Raw News is not a genuine news source. No...</td>\n",
       "      <td>In March 2022, as Russia continued its attack ...</td>\n",
       "      <td>Real Raw News is not a genuine news source. No...</td>\n",
       "      <td>Real Raw News is not a genuine news source. No...</td>\n",
       "      <td>A disclaimer on Real Raw News states that the ...</td>\n",
       "      <td>In March 2022, as Russia continued its attack...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.650751e+09</td>\n",
       "      <td>In September 2021, an initial news release ann...</td>\n",
       "      <td>true</td>\n",
       "      <td>a typo appeared to suggest that first-term con...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In September 2021, some internet users gleeful...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The article of impeachment against Harris accu...</td>\n",
       "      <td>On Sept. 24, for example, political reporter ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                         allegation  \\\n",
       "0  1.650725e+09  Valentine's Day was invented by greeting card ...   \n",
       "1  1.650726e+09  A video shows truckers from South Carolina on ...   \n",
       "2  1.650725e+09  An online advertisement revealed an unusual or...   \n",
       "3  1.650725e+09  Russia bombed a Biden-owned villa and several ...   \n",
       "4  1.650751e+09  In September 2021, an initial news release ann...   \n",
       "\n",
       "     evaluation                                                 T5  \\\n",
       "0         false  the holiday falls yearly on february 14. some ...   \n",
       "1  miscaptioned  video shows truckers from south carolina in th...   \n",
       "2         false  for at least several years, online advertiseme...   \n",
       "3         false  a fictitious article falsely claimed a villa o...   \n",
       "4          true  a typo appeared to suggest that first-term con...   \n",
       "\n",
       "                                                BERT  \\\n",
       "0  And he called her his “very gentle Valentine.”...   \n",
       "1                                                NaN   \n",
       "2   Azrieli Center, 26 Harokmim St., Holon, Israel.”   \n",
       "3  Real Raw News is not a genuine news source. No...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                                BART  \\\n",
       "0  The idea that Valentine’s Day, which falls yea...   \n",
       "1  In January 2022, as hundreds of truckers drove...   \n",
       "2  For at least several years, online advertiseme...   \n",
       "3  In March 2022, as Russia continued its attack ...   \n",
       "4  In September 2021, some internet users gleeful...   \n",
       "\n",
       "                                               XLNet  \\\n",
       "0  And he called her his “very gentle Valentine.”...   \n",
       "1                                                NaN   \n",
       "2   Azrieli Center, 26 Harokmim St., Holon, Israel.”   \n",
       "3  Real Raw News is not a genuine news source. No...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                                GTP2  \\\n",
       "0  And he called her his “very gentle Valentine.”...   \n",
       "1                                                NaN   \n",
       "2   Azrieli Center, 26 Harokmim St., Holon, Israel.”   \n",
       "3  Real Raw News is not a genuine news source. No...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                       textRank_sumy  \\\n",
       "0  Although one can’t factually argue the holiday...   \n",
       "1  In January 2022, as hundreds of truckers drove...   \n",
       "2  A spokesperson for Farmers Insurance responded...   \n",
       "3  A disclaimer on Real Raw News states that the ...   \n",
       "4  The article of impeachment against Harris accu...   \n",
       "\n",
       "                                       textRank_nltk  \n",
       "0   Although one can’t factually argue the holida...  \n",
       "1   In January 2022, as hundreds of truckers drov...  \n",
       "2   A spokesperson for Farmers Insurance responde...  \n",
       "3   In March 2022, as Russia continued its attack...  \n",
       "4   On Sept. 24, for example, political reporter ...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('summarization.csv', index_col=0)\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
