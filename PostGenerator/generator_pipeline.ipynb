{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4abe034c",
   "metadata": {},
   "source": [
    "# Generator\n",
    "\n",
    "Ferramenta que pega no conteudo das noticias (em formato json) e gera posts.\n",
    "Para isso s√£o executados os seguintes passos:\n",
    "\n",
    "   1) Obten√ß√£o do core do texto:\n",
    "      -  T5 \n",
    "      - Allegation sentence\n",
    "\n",
    "   2) Refor√ßo de emo√ß√£o: \n",
    "      - Emojis\n",
    "      - Hahstags\n",
    "      - Caps lock\n",
    "      - interjection\n",
    "\n",
    "   3) Refor√ßo de personalidade\n",
    "      Avalia o processo mental predominante, recorrendo ao menTFIDF;\n",
    "      Refor√ßa o processo mental predominante.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "544627d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mariabarbosa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## Importes necessarios\n",
    "\n",
    "import os\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "from men_tf_idf import MenTFIDF\n",
    "\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nrclex import NRCLex\n",
    "import nltk\n",
    "import spacy\n",
    "import json\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14ba522c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "verificar se um post (em formato de string) tem um formato v√°lido de um microblog:\n",
    "     1) 0 < n√∫mero de carateres ‚â• 280 \n",
    "'''\n",
    "\n",
    "\n",
    "def verify_length(post):\n",
    "    content = \"\"\n",
    "\n",
    "    for i in post:\n",
    "        if type(post[i]) == str:\n",
    "            content = content + \" \" + post[i] \n",
    "        else:\n",
    "            content = content + \" \" + ' '.join(post[i]) \n",
    "    if len(content)>280:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91beee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TextCore import T5 \n",
    "\n",
    "def allegation_sentence (data):\n",
    "    claim = \"It's \" + data['evaluation'] + ' that '+ data['allegation']\n",
    "    return claim\n",
    "\n",
    "\n",
    "def post_core (data, core_model):\n",
    "    \n",
    "    if core_model == \"t5\":\n",
    "        core = T5.T5_summarization_model(' '.join(data['postText']))\n",
    "    else:\n",
    "        core = allegation_sentence (data)\n",
    "    return core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e1bc62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from emotions import hashtags_generetor\n",
    "from emotions import emojis\n",
    "from emotions import interjections\n",
    "from emotions import caps_lock\n",
    "\n",
    "\n",
    "\n",
    "def add_emotion(data,core, emoji , hashtag , capslock  , intejections ):\n",
    "    post = {}\n",
    "    post['url'] = data['url']\n",
    "    ## N√£o adiciona mais conteudo por isso n√£o √© necessario verificar tamanho\n",
    "    if capslock: \n",
    "        post['core']  = caps_lock.capslock(core)\n",
    "    else:\n",
    "        post['core'] = core\n",
    "\n",
    "    post['emojis'] = []\n",
    "    if emoji and verify_length(post): \n",
    "        post['emojis'].append(emojis.emojis_sellection(data['evaluation']))\n",
    "\n",
    "    post['hashtags'] = []\n",
    "    if hashtag and verify_length(post):\n",
    "        hashtag_list = hashtags_generetor.generateHashtags(data['postText'])\n",
    "        for h in hashtag_list:\n",
    "            post['hashtags'].append('#'+h)\n",
    "    \n",
    "    post['interjections'] = [] \n",
    "    if intejections and verify_length(post): \n",
    "        post['interjections'].append(interjections.interjections(data['evaluation']))\n",
    "\n",
    "    if emoji and verify_length(post): \n",
    "        #adicionar mais emojis se ainda\n",
    "        seed = random.randint(0,4)\n",
    "        while verify_length(post) and seed < 4:\n",
    "            post['emojis'].append(emojis.emojis_sellection(data['evaluation']))\n",
    "            seed = seed + 1\n",
    "    ## Post fica com ordem alaetoria.\n",
    "   # core_with_emotion = random.shuffle(post) \n",
    "\n",
    "    return post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "578dac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Core_generator (data,core_model='t5', emoji = 1, hashtag = 1, caps_lock =1 , intejections = 1):\n",
    "  '''\n",
    "  data -> Conteudo do json file\n",
    "  '''\n",
    "  \n",
    "  ## Obter o core do post\n",
    "  core = post_core (data, core_model)\n",
    "  \n",
    "  ## dar emo√ß√£o ao core\n",
    "  core_with_emotion = add_emotion(data, core, emoji , hashtag , caps_lock  , intejections )\n",
    "\n",
    "  ## dar personalidade\n",
    "  final_post = core_with_emotion  \n",
    "\n",
    "  return final_post  \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a74b8b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creat_post_structure(post_data):\n",
    "\n",
    "    lista_post = [post_data['core']]\n",
    "\n",
    "    for h in  post_data['hashtags']:\n",
    "        lista_post.append(h)\n",
    "\n",
    "    for e in  post_data['emojis']:\n",
    "        lista_post.append(e)\n",
    "\n",
    "    for i in  post_data['interjections']:\n",
    "        lista_post.append(i)\n",
    "\n",
    "    random.shuffle(lista_post) \n",
    "\n",
    "    lista_post.append(post_data['url'])\n",
    "    post = ' '.join(lista_post)\n",
    "    return post \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e0d9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "##### Read the json files with de news\n",
    "for file in os.listdir(\"../../extractors/Snopes/exemplo\"):\n",
    "    with open(\"../../extractors/Snopes/exemplo/\" + file) as f:\n",
    "        data = json.load(f)\n",
    "        f.close()\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "240e3ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('MentaLex/MentaLex_synonyms.csv')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "539aa8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Fun√ß√µes que permitem refor√ßar o processo mental\n",
    "Isto feito atraves de substitui√ß√µes de palavras que n√£o pertencem ao leixco desse tra√ßo mental, por sinonimos que perten√ßam \n",
    "'''\n",
    "\n",
    "def Schizoid (sentence):\n",
    "    words_sentence = []\n",
    "    new_sentence = []\n",
    "    for word in sentence.split(' '):\n",
    "        words_sentence.append(word)\n",
    "\n",
    "    for w in words_sentence: \n",
    "        if w not in list_shizoid:\n",
    "            ''' \n",
    "            Analisar os sinonimos de w e ver se algum deles esta na lista. Se estiver substituir\n",
    "            '''\n",
    "\n",
    "            syn = synonyms(w)\n",
    "            f = 0\n",
    "            for s in syn:\n",
    "                if s in list_shizoid:\n",
    "                    f = 1\n",
    "                    new_sentence.append(s)\n",
    "                    break\n",
    "            if f==0:\n",
    "                 new_sentence.append(w)\n",
    "                \n",
    "        else:\n",
    "            new_sentence.append(w)\n",
    "\n",
    "            \n",
    "    return new_sentence\n",
    "\n",
    "def Paranoid (sentence):\n",
    "    words_sentence = []\n",
    "    new_sentence = []\n",
    "    for word in sentence.split(' '):\n",
    "        words_sentence.append(word)\n",
    "\n",
    "    for w in words_sentence: \n",
    "        if w not in list_paranoid:\n",
    "            ''' \n",
    "            Analisar os sinonimos de w e ver se algum deles esta na lista. Se estiver substituir\n",
    "            '''\n",
    "\n",
    "            syn = synonyms(w)\n",
    "            f = 0\n",
    "            for s in syn:\n",
    "                if s in list_paranoid:\n",
    "                    f = 1\n",
    "                    new_sentence.append(s)\n",
    "                    break\n",
    "            if f==0:\n",
    "                 new_sentence.append(w)\n",
    "                \n",
    "        else:\n",
    "            new_sentence.append(w)\n",
    "\n",
    "            \n",
    "    return new_sentence\n",
    "\n",
    "\n",
    "\n",
    "def Neurotic (sentence):\n",
    "    words_sentence = []\n",
    "    new_sentence = []\n",
    "    for word in sentence.split(' '):\n",
    "        words_sentence.append(word)\n",
    "\n",
    "    for w in words_sentence: \n",
    "        if w not in list_neurotic:\n",
    "            ''' \n",
    "            Analisar os sinonimos de w e ver se algum deles esta na lista. Se estiver substituir\n",
    "            Verificar que a palavra n esteja nas que n√£o se deve incluir\n",
    "            '''\n",
    "\n",
    "            syn = synonyms(w)\n",
    "            f = 0\n",
    "            for s in syn:\n",
    "                if s in list_neurotic:\n",
    "                    f = 1\n",
    "                    new_sentence.append(s)\n",
    "                    break\n",
    "            if f==0:\n",
    "                 new_sentence.append(w)\n",
    "                \n",
    "        else:\n",
    "            new_sentence.append(w)\n",
    "\n",
    "            \n",
    "    return new_sentence\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "list_paranoid = list(df[df['classification'] == 'Paranoid']['word'])\n",
    "list_neurotic = list(df[df['classification'] == 'Neuroticism']['word'])\n",
    "list_shizoid = list(df[df['classification'] == 'Schizoid']['word'])\n",
    "\n",
    "\n",
    "def synonyms (word):\n",
    "\n",
    "    synonyms = []\n",
    "\n",
    "    for syn in wn.synsets(word):\n",
    "            for l in syn.lemmas():\n",
    "                doc = nlp(l.name())\n",
    "                s = [ str(token) for token in doc ] \n",
    "                synonyms = synonyms + s\n",
    "                    \n",
    "    # print(set(synonyms_traits))\n",
    "\n",
    "    return synonyms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ebdab6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "''''\n",
    "Algoritmo geral\n",
    "'''\n",
    "\n",
    "def postGenerator (filePath, model='allegation_Sentence'):\n",
    "    with open(filePath) as file:\n",
    "        data = json.load(file)\n",
    "        file.close()\n",
    "\n",
    "    #Gerar Text Core\n",
    "    p = Core_generator(data, model)\n",
    "    # Adicionar a emo√ß√£o\n",
    "    post_with_emotion = creat_post_structure(p)\n",
    "\n",
    "    #verificar processo mental predominante pelo calculo do valor TFIDF\n",
    "    menTFIDF  = MenTFIDF() \n",
    "    menTFIDF.set_text(post_with_emotion)\n",
    "    frq = menTFIDF.em_frequencies\n",
    "\n",
    "    ## ordenar por valor de frequencia\n",
    "    mental_tfidf_frequencies = {k: v for k, v in sorted(frq.items(), key=lambda item: item[1], reverse=True)}\n",
    "    \n",
    "        # extrair tra√ßo mental predominante\n",
    "    mental_process = list(mental_tfidf_frequencies.keys())[0]\n",
    "\n",
    "    #De acordo com tra√ßo mental predominante refor√ßar esse tra√ßo\n",
    "    if mental_process == \"Neurocitism\":\n",
    "\n",
    "        final_post  = Neurotic (post_with_emotion)\n",
    "    elif mental_process == \"Paranoid\":\n",
    "        final_post = Paranoid (post_with_emotion)\n",
    "    else:\n",
    "        final_post = Schizoid(post_with_emotion)\n",
    "\n",
    "\n",
    "    return (' '.join(final_post))  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fc948d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snopes_extraction_1663673769.393436.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"üòÇ üëÖ #john It's true that A VIDEO of a yearn SNAKE ‚Äústanding up‚Äù and lift its head high into the strain follow real. #black oh üë≠ #snake üèà #cobra ‚îÇ https://www.snopes.com/fact-check/snake-standing-up-video/?utm_term=Autofeed&utm_medium=Social&utm_source=Twitter#Echobox=1663406617\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "for file in os.listdir(\"../../extractors/Snopes/news\")[2:3]:\n",
    "        with open(\"../../extractors/Snopes/news/\" + file) as f:\n",
    "            data = json.load(f)\n",
    "            f.close()\n",
    "            for a in ['allegation_Sentence', 't5']:\n",
    "                p = generator(data, a)\n",
    "                post_with_emotion = creat_post_structure(p)\n",
    "                pers= personality (post_with_emotion)\n",
    "               \n",
    "'''\n",
    "\n",
    "file = (os.listdir(\"../../extractors/Snopes/news\")[2])\n",
    "print(file)\n",
    "postGenerator (\"../../extractors/Snopes/news/\" + file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7211518",
   "metadata": {},
   "outputs": [],
   "source": [
    "''''\n",
    "i = 0\n",
    "lista = []\n",
    "for file in os.listdir(\"../../extractors/Snopes/news\"):\n",
    "        with open(\"../../extractors/Snopes/news/\" + file) as f:\n",
    "            data = json.load(f)\n",
    "            f.close()\n",
    "            for a in ['allegation_Sentence', 't5']:\n",
    "                p = generator(data, a)\n",
    "                post_with_emotion = creat_post_structure(p)\n",
    "                pers= personality (post_with_emotion)\n",
    "                lista.append([i,a,post_with_emotion, pers])\n",
    "                i = i +1\n",
    "\n",
    "df = pd.DataFrame(lista, columns=['id', 'aproach', 'post', 'personality'])\n",
    "print(df)\n",
    "\n",
    "\n",
    "\n",
    "##########################\n",
    "\n",
    "df.to_csv('post_news.csv', index = False)\n",
    "import pandas as pd\n",
    "df = pd.read_csv('post_news.csv')\n",
    "\n",
    "df['personality'][10]\n",
    "'''    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f7c8301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nrclex import NRCLex\n",
    "\n",
    "def emotionsNRClex(text):\n",
    "  \n",
    "    emotion = NRCLex(text)\n",
    "    emotions_list = emotion.affect_frequencies\n",
    "    return emotions_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2c0b35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
