{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4abe034c",
   "metadata": {},
   "source": [
    "# Generator\n",
    "\n",
    "Ferramenta que pega no conteudo das noticias (em formato json) e gera posts.\n",
    "Para isso são executados os seguintes passos:\n",
    "\n",
    "   1) Obtenção do core do texto:\n",
    "      -  T5 \n",
    "      - Allegation sentence\n",
    "\n",
    "   2) Reforço de emoção: \n",
    "      - Emojis\n",
    "      - Hahstags\n",
    "      - Caps lock\n",
    "      - interjection\n",
    "\n",
    "   3) Reforço de personalidade\n",
    "      Avalia o processo mental predominante, recorrendo ao menTFIDF;\n",
    "      Reforça o processo mental predominante.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "544627d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mariabarbosa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## Importes necessarios\n",
    "\n",
    "import os\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "from men_tf_idf import MenTFIDF\n",
    "\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nrclex import NRCLex\n",
    "import nltk\n",
    "import spacy\n",
    "import json\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14ba522c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "verificar se um post (em formato de string) tem um formato válido de um microblog:\n",
    "     1) 0 < número de carateres ≥ 280 \n",
    "'''\n",
    "\n",
    "\n",
    "def verify_length(post):\n",
    "    content = \"\"\n",
    "\n",
    "    for i in post:\n",
    "        if type(post[i]) == str:\n",
    "            content = content + \" \" + post[i] \n",
    "        else:\n",
    "            content = content + \" \" + ' '.join(post[i]) \n",
    "    if len(content)>280:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91beee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TextCore import T5 \n",
    "\n",
    "def allegation_sentence (data):\n",
    "    claim = \"It's \" + data['evaluation'] + ' that '+ data['allegation']\n",
    "    return claim\n",
    "\n",
    "\n",
    "def post_core (data, core_model):\n",
    "    \n",
    "    if core_model == \"t5\":\n",
    "        core = T5.T5_summarization_model(' '.join(data['postText']))\n",
    "    else:\n",
    "        core = allegation_sentence (data)\n",
    "    return core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e1bc62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from emotions import hashtags_generetor\n",
    "from emotions import emojis\n",
    "from emotions import interjections\n",
    "from emotions import caps_lock\n",
    "\n",
    "\n",
    "\n",
    "def add_emotion(data,core, emoji , hashtag , capslock  , intejections ):\n",
    "    post = {}\n",
    "    post['url'] = data['url']\n",
    "    ## Não adiciona mais conteudo por isso não é necessario verificar tamanho\n",
    "    if capslock: \n",
    "        post['core']  = caps_lock.capslock(core)\n",
    "    else:\n",
    "        post['core'] = core\n",
    "\n",
    "    post['emojis'] = []\n",
    "    if emoji and verify_length(post): \n",
    "        post['emojis'].append(emojis.emojis_sellection(data['evaluation']))\n",
    "\n",
    "    post['hashtags'] = []\n",
    "    if hashtag and verify_length(post):\n",
    "        hashtag_list = hashtags_generetor.generateHashtags(data['postText'])\n",
    "        for h in hashtag_list:\n",
    "            post['hashtags'].append('#'+h)\n",
    "    \n",
    "    post['interjections'] = [] \n",
    "    if intejections and verify_length(post): \n",
    "        post['interjections'].append(interjections.interjections(data['evaluation']))\n",
    "\n",
    "    if emoji and verify_length(post): \n",
    "        #adicionar mais emojis se ainda\n",
    "        seed = random.randint(0,4)\n",
    "        while verify_length(post) and seed < 4:\n",
    "            post['emojis'].append(emojis.emojis_sellection(data['evaluation']))\n",
    "            seed = seed + 1\n",
    "    ## Post fica com ordem alaetoria.\n",
    "   # core_with_emotion = random.shuffle(post) \n",
    "\n",
    "    return post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "578dac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Core_generator (data,core_model='t5', emoji = 1, hashtag = 1, caps_lock =1 , intejections = 1):\n",
    "  '''\n",
    "  data -> Conteudo do json file\n",
    "  '''\n",
    "  \n",
    "  ## Obter o core do post\n",
    "  core = post_core (data, core_model)\n",
    "  \n",
    "  ## dar emoção ao core\n",
    "  core_with_emotion = add_emotion(data, core, emoji , hashtag , caps_lock  , intejections )\n",
    "\n",
    "  ## dar personalidade\n",
    "  final_post = core_with_emotion  \n",
    "\n",
    "  return final_post  \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a74b8b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creat_post_structure(post_data):\n",
    "\n",
    "    lista_post = [post_data['core']]\n",
    "\n",
    "    for h in  post_data['hashtags']:\n",
    "        lista_post.append(h)\n",
    "\n",
    "    for e in  post_data['emojis']:\n",
    "        lista_post.append(e)\n",
    "\n",
    "    for i in  post_data['interjections']:\n",
    "        lista_post.append(i)\n",
    "\n",
    "    random.shuffle(lista_post) \n",
    "\n",
    "    lista_post.append(post_data['url'])\n",
    "    post = ' '.join(lista_post)\n",
    "    return post \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e0d9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "##### Read the json files with de news\n",
    "for file in os.listdir(\"../../extractors/Snopes/exemplo\"):\n",
    "    with open(\"../../extractors/Snopes/exemplo/\" + file) as f:\n",
    "        data = json.load(f)\n",
    "        f.close()\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "240e3ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('MentaLex/MentaLex_synonyms.csv')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "539aa8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Funções que permitem reforçar o processo mental\n",
    "Isto feito atraves de substituições de palavras que não pertencem ao leixco desse traço mental, por sinonimos que pertençam \n",
    "'''\n",
    "\n",
    "def Schizoid (sentence):\n",
    "    words_sentence = []\n",
    "    new_sentence = []\n",
    "    for word in sentence.split(' '):\n",
    "        words_sentence.append(word)\n",
    "\n",
    "    for w in words_sentence: \n",
    "        if w not in list_shizoid:\n",
    "            ''' \n",
    "            Analisar os sinonimos de w e ver se algum deles esta na lista. Se estiver substituir\n",
    "            '''\n",
    "\n",
    "            syn = synonyms(w)\n",
    "            f = 0\n",
    "            for s in syn:\n",
    "                if s in list_shizoid:\n",
    "                    f = 1\n",
    "                    new_sentence.append(s)\n",
    "                    break\n",
    "            if f==0:\n",
    "                 new_sentence.append(w)\n",
    "                \n",
    "        else:\n",
    "            new_sentence.append(w)\n",
    "\n",
    "            \n",
    "    return new_sentence\n",
    "\n",
    "def Paranoid (sentence):\n",
    "    words_sentence = []\n",
    "    new_sentence = []\n",
    "    for word in sentence.split(' '):\n",
    "        words_sentence.append(word)\n",
    "\n",
    "    for w in words_sentence: \n",
    "        if w not in list_paranoid:\n",
    "            ''' \n",
    "            Analisar os sinonimos de w e ver se algum deles esta na lista. Se estiver substituir\n",
    "            '''\n",
    "\n",
    "            syn = synonyms(w)\n",
    "            f = 0\n",
    "            for s in syn:\n",
    "                if s in list_paranoid:\n",
    "                    f = 1\n",
    "                    new_sentence.append(s)\n",
    "                    break\n",
    "            if f==0:\n",
    "                 new_sentence.append(w)\n",
    "                \n",
    "        else:\n",
    "            new_sentence.append(w)\n",
    "\n",
    "            \n",
    "    return new_sentence\n",
    "\n",
    "\n",
    "\n",
    "def Neurotic (sentence):\n",
    "    words_sentence = []\n",
    "    new_sentence = []\n",
    "    for word in sentence.split(' '):\n",
    "        words_sentence.append(word)\n",
    "\n",
    "    for w in words_sentence: \n",
    "        if w not in list_neurotic:\n",
    "            ''' \n",
    "            Analisar os sinonimos de w e ver se algum deles esta na lista. Se estiver substituir\n",
    "            Verificar que a palavra n esteja nas que não se deve incluir\n",
    "            '''\n",
    "\n",
    "            syn = synonyms(w)\n",
    "            f = 0\n",
    "            for s in syn:\n",
    "                if s in list_neurotic:\n",
    "                    f = 1\n",
    "                    new_sentence.append(s)\n",
    "                    break\n",
    "            if f==0:\n",
    "                 new_sentence.append(w)\n",
    "                \n",
    "        else:\n",
    "            new_sentence.append(w)\n",
    "\n",
    "            \n",
    "    return new_sentence\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "list_paranoid = list(df[df['classification'] == 'Paranoid']['word'])\n",
    "list_neurotic = list(df[df['classification'] == 'Neuroticism']['word'])\n",
    "list_shizoid = list(df[df['classification'] == 'Schizoid']['word'])\n",
    "\n",
    "\n",
    "def synonyms (word):\n",
    "\n",
    "    synonyms = []\n",
    "\n",
    "    for syn in wn.synsets(word):\n",
    "            for l in syn.lemmas():\n",
    "                doc = nlp(l.name())\n",
    "                s = [ str(token) for token in doc ] \n",
    "                synonyms = synonyms + s\n",
    "                    \n",
    "    # print(set(synonyms_traits))\n",
    "\n",
    "    return synonyms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ebdab6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "''''\n",
    "Algoritmo geral\n",
    "'''\n",
    "\n",
    "def postGenerator (filePath, model='allegation_Sentence'):\n",
    "    with open(filePath) as file:\n",
    "        data = json.load(file)\n",
    "        file.close()\n",
    "\n",
    "    #Gerar Text Core\n",
    "    p = Core_generator(data, model)\n",
    "    # Adicionar a emoção\n",
    "    post_with_emotion = creat_post_structure(p)\n",
    "\n",
    "    #verificar processo mental predominante pelo calculo do valor TFIDF\n",
    "    menTFIDF  = MenTFIDF() \n",
    "    menTFIDF.set_text(post_with_emotion)\n",
    "    frq = menTFIDF.em_frequencies\n",
    "\n",
    "    ## ordenar por valor de frequencia\n",
    "    mental_tfidf_frequencies = {k: v for k, v in sorted(frq.items(), key=lambda item: item[1], reverse=True)}\n",
    "    \n",
    "        # extrair traço mental predominante\n",
    "    mental_process = list(mental_tfidf_frequencies.keys())[0]\n",
    "\n",
    "    #De acordo com traço mental predominante reforçar esse traço\n",
    "    if mental_process == \"Neurocitism\":\n",
    "\n",
    "        final_post  = Neurotic (post_with_emotion)\n",
    "    elif mental_process == \"Paranoid\":\n",
    "        final_post = Paranoid (post_with_emotion)\n",
    "    else:\n",
    "        final_post = Schizoid(post_with_emotion)\n",
    "\n",
    "\n",
    "    return (' '.join(final_post))  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fc948d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snopes_extraction_1663673769.393436.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"😂 👅 #john It's true that A VIDEO of a yearn SNAKE “standing up” and lift its head high into the strain follow real. #black oh 👭 #snake 🏈 #cobra │ https://www.snopes.com/fact-check/snake-standing-up-video/?utm_term=Autofeed&utm_medium=Social&utm_source=Twitter#Echobox=1663406617\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "for file in os.listdir(\"../../extractors/Snopes/news\")[2:3]:\n",
    "        with open(\"../../extractors/Snopes/news/\" + file) as f:\n",
    "            data = json.load(f)\n",
    "            f.close()\n",
    "            for a in ['allegation_Sentence', 't5']:\n",
    "                p = generator(data, a)\n",
    "                post_with_emotion = creat_post_structure(p)\n",
    "                pers= personality (post_with_emotion)\n",
    "               \n",
    "'''\n",
    "\n",
    "file = (os.listdir(\"../../extractors/Snopes/news\")[2])\n",
    "print(file)\n",
    "postGenerator (\"../../extractors/Snopes/news/\" + file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7211518",
   "metadata": {},
   "outputs": [],
   "source": [
    "''''\n",
    "i = 0\n",
    "lista = []\n",
    "for file in os.listdir(\"../../extractors/Snopes/news\"):\n",
    "        with open(\"../../extractors/Snopes/news/\" + file) as f:\n",
    "            data = json.load(f)\n",
    "            f.close()\n",
    "            for a in ['allegation_Sentence', 't5']:\n",
    "                p = generator(data, a)\n",
    "                post_with_emotion = creat_post_structure(p)\n",
    "                pers= personality (post_with_emotion)\n",
    "                lista.append([i,a,post_with_emotion, pers])\n",
    "                i = i +1\n",
    "\n",
    "df = pd.DataFrame(lista, columns=['id', 'aproach', 'post', 'personality'])\n",
    "print(df)\n",
    "\n",
    "\n",
    "\n",
    "##########################\n",
    "\n",
    "df.to_csv('post_news.csv', index = False)\n",
    "import pandas as pd\n",
    "df = pd.read_csv('post_news.csv')\n",
    "\n",
    "df['personality'][10]\n",
    "'''    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f7c8301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nrclex import NRCLex\n",
    "\n",
    "def emotionsNRClex(text):\n",
    "  \n",
    "    emotion = NRCLex(text)\n",
    "    emotions_list = emotion.affect_frequencies\n",
    "    return emotions_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2c0b35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
