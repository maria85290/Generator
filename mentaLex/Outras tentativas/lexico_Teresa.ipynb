{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import nltk\n",
    "\n",
    "df = pd.read_csv('../essay.csv').drop(columns='split')  # removes split column\n",
    "df = df.replace({'n': 0, 'y': 1})  # turns y/n into binary\n",
    "\n",
    "df.insert(1, 'year', df['#AUTHID'].apply(lambda id: id[0:4]))  # creates new column with year info\n",
    "df['#AUTHID'] = df['#AUTHID'].apply(lambda id: id[0:-4])  # removes '.txt' from IDs\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "# moved the imports, for readability\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from autocorrect import Speller\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"\n",
    "    return WORDNET POS compliance to WORDENT lemmatization (a,n,r,v)\n",
    "    \"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "\n",
    "def treat_data(sentence):\n",
    "    s_tokens = word_tokenize(sentence)\n",
    "    spell = Speller(\"en\")\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    for i in range(len(s_tokens)):\n",
    "        # correct the word spelling\n",
    "        s_tokens[i] = spell(str.lower(s_tokens[i]))\n",
    "\n",
    "        # postag\n",
    "    pt = pos_tag(s_tokens)\n",
    "    word_list = []\n",
    "    for tag in pt:\n",
    "        w_temp = lemmatizer.lemmatize(word=tag[0], pos=get_wordnet_pos(tag[1]))\n",
    "        if w_temp not in stop_words and w_temp.isalnum() and not w_temp.isdigit():\n",
    "            word_list.append(w_temp)\n",
    "    return word_list\n",
    "\n",
    "\n",
    "def text2words(text):\n",
    "    clean_words = []\n",
    "    sent_list = sent_tokenize(text)\n",
    "    for sent in sent_list:\n",
    "        clean_words += treat_data(sent)\n",
    "    return clean_words\n",
    "\n",
    "\n",
    "df.insert(3, 'WORDS', df['TEXT'].apply(text2words))\n",
    "df.to_csv('out.csv', index=False)\n",
    "# df.to_json('pretty.json', orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "ocean_words = pd.read_csv('out.csv', usecols=['#AUTHID', 'WORDS', 'cEXT', 'cNEU', 'cAGR', 'cCON', 'cOPN'],\n",
    "                          dtype={'cEXT': bool, 'cNEU': bool, 'cAGR': bool, 'cCON': bool, 'cOPN': bool})\n",
    "ocean_words['WORDS'] = ocean_words['WORDS'].apply(ast.literal_eval)\n",
    "\n",
    "words = ocean_words['WORDS'].explode().unique().tolist()\n",
    "\n",
    "\n",
    "def dummy_tokenizer(doc):\n",
    "    return doc\n",
    "\n",
    "\n",
    "def tfidf_trait(df, trait, b):\n",
    "    small_df = df[df[trait] == b]\n",
    "    tfidf_model = TfidfVectorizer(vocabulary=words, tokenizer=dummy_tokenizer, preprocessor=dummy_tokenizer,\n",
    "                                  token_pattern=None, analyzer='word')\n",
    "    fitted = tfidf_model.fit_transform(small_df.WORDS).todense()\n",
    "    df_tfidf = pd.DataFrame(fitted)\n",
    "    df_tfidf.columns = sorted(tfidf_model.vocabulary_)\n",
    "    tfidf_mean = df_tfidf.mean()\n",
    "    return dict(tfidf_mean)\n",
    "\n",
    "\n",
    "lexicon = pd.DataFrame(words, columns=['words'])\n",
    "for trait in ['cEXT', 'cNEU', 'cAGR', 'cCON', 'cOPN']:\n",
    "    for b in [False, True]:\n",
    "        col_name = '{0}_{1}'.format(trait, b)\n",
    "        lexicon[col_name] = lexicon['words'].map(tfidf_trait(ocean_words, trait, b))\n",
    "    col_perc_name = '{0}_perc'.format(trait)\n",
    "    lexicon[col_perc_name] = (lexicon['{0}_True'.format(trait)] - lexicon['{0}_False'.format(trait)] + 1) / 2\n",
    "\n",
    "print(lexicon[['words', 'cEXT_perc', 'cNEU_perc', 'cAGR_perc', 'cCON_perc', 'cOPN_perc']])\n",
    "lexicon.to_csv('lexicon.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    words cEXT_polarity cNEU_polarity cAGR_polarity cCON_polarity  \\\n",
      "0     get       neutral       neutral       neutral       neutral   \n",
      "1    back       neutral       neutral       neutral       neutral   \n",
      "2   class       neutral       neutral       neutral       neutral   \n",
      "3  decide       neutral       neutral       neutral       neutral   \n",
      "4   start       neutral      negative      positive       neutral   \n",
      "\n",
      "  cOPN_polarity  \n",
      "0       neutral  \n",
      "1       neutral  \n",
      "2       neutral  \n",
      "3       neutral  \n",
      "4       neutral  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "## Importar o ficheiro csv\n",
    "df = pd.read_csv('lexicon_pol.csv', index_col=False,sep=',', encoding='ISO 8859-1')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = []\n",
    "O = []\n",
    "N = []\n",
    "C = []\n",
    "E = []\n",
    "for row in df.itertuples():\n",
    "    if row.cEXT_polarity == 'positive':\n",
    "        E.append(row.words)\n",
    "    \n",
    "    if row.cAGR_polarity == 'positive':\n",
    "        A.append(row.words)\n",
    "\n",
    "    if row.cOPN_polarity == 'positive':\n",
    "        O.append(row.words)\n",
    "\n",
    "    if row.cCON_polarity == 'positive':\n",
    "        C.append(row.words)\n",
    "\n",
    "    if row.cNEU_polarity == 'positive':\n",
    "        N.append(row.words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparar resultados com a nossa abordagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "f = open('../MentaLex_synonyms.json')\n",
    "  \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adopt', 'naming', 'attend', 'abide', 'bottom', 'interior', 'assist', 'anguish', 'recognise', 'certain', 'fixed', 'range', 'assignment', 'cast', 'easygoing', 'border', 'mess', 'rambling', 'fall', 'think', 'household', 'band', 'aim', 'address', 'generate', 'derive', 'begin', 'assure', 'check', 'bust', 'beat', 'fashion', 'start', 'exit', 'believe'}\n",
      "{'naming', 'payoff', 'conduct', 'content', 'order', 'catch', 'bit', 'book', 'death', 'cum', 'emphasize', 'determined', 'correct', 'groom', 'sack', 'old', 'guess', 'cook', 'form', 'gain', 'clock', 'accent', 'acquaintance', 'articulate', 'cause', 'accept', 'bunch', 'feeling', 'guide', 'point', 'designate', 'smart', 'derive', 'fatigue', 'dot', 'cut', 'daylight'}\n",
      "{'conclude', 'case', 'aspect', 'attend', 'engagement', 'judge', 'exploit', 'incur', 'backyard', 'event', 'adept', 'catch', 'crop', 'batch', 'blow', 'appreciation', 'anticipate', 'battlefield', 'accommodate', 'beloved', 'dreaded', 'acquire', 'day', 'chase', 'plow', 'daughter', 'deform', 'conclusion', 'click', 'chill', 'focusing', 'everlasting', 'expert', 'block', 'ace', 'fix', 'hump', 'lookout', 'de', 'advert', 'pertain', 'enter', 'consider', 'astonish', 'concentrate', 'lick', 'deal', 'raw', 'battle', 'beneficial', 'extend', 'eyeball', 'cool', 'alien', 'moment', 'bind', 'amount', '2nd', 'grief', 'assurance', 'airplane', 'area', 'conflict', 'appear'}\n"
     ]
    }
   ],
   "source": [
    "print(set(data['paranoid']) & set(A))\n",
    "print(set(data['schizoid']) & set(O))\n",
    "print(set(data['Neuroticism']) & set(N))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e73066c94f8e0a03dc010f90f9bfe8189274fe2970ef1c1470299afeb680a60a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('py3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
