{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractive summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/py3/lib/python3.9/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.0.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "# Load Packages\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sumy library\n",
    "def text_rank (text):\n",
    "    parser = PlaintextParser.from_string(text,Tokenizer(\"english\"))\n",
    "\n",
    "    # Summarize using sumy TextRank\n",
    "    summarizer = TextRankSummarizer()\n",
    "    summary =summarizer(parser.document,1)\n",
    "    text_summary=\"\"\n",
    "\n",
    "    for sentence in summary:\n",
    "        text_summary +=str(sentence)\n",
    "\n",
    "    return (text_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## using NLTK\n",
    "def nltk_extractive_sumy(text):\n",
    "\n",
    "    stopWords = set(stopwords.words(\"english\"))\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    freqTable = dict()  ## Frequencia das palavras\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if word in stopWords:\n",
    "            continue\n",
    "        if word in freqTable:   \n",
    "            freqTable[word] += 1\n",
    "        else:\n",
    "            freqTable[word] = 1\n",
    "\n",
    "\n",
    "\n",
    "    sentences = sent_tokenize(text)  \n",
    "    sentenceValue = dict() ## Importancia das frases\n",
    "\n",
    "    for sentence in sentences:\n",
    "        for word, freq in freqTable.items():\n",
    "            if word in sentence.lower():\n",
    "              if sentence in sentenceValue:\n",
    "                sentenceValue[sentence] += freq\n",
    "              else:\n",
    "                sentenceValue[sentence] = freq\n",
    "\n",
    "    summary = ''\n",
    "\n",
    "    sentenceValue = dict(sorted(sentenceValue.items(), key=lambda item: item[1], reverse=True))\n",
    "   \n",
    "    for sentence in sentenceValue:\n",
    "\n",
    "        if len(summary)<230:\n",
    "        #and (sentenceValue[sentence] > (1.40 * average)):\n",
    "            summary += \" \" + sentence\n",
    "\n",
    "    return (summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guardar no dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "\n",
    "data = pd.DataFrame()\n",
    "\n",
    "for file in os.listdir(\"../../../extractors/Snopes/extractions\")[:100]:\n",
    "    with open(\"../../../extractors/Snopes/extractions/\" + file) as f:\n",
    "            d = json.load(f)\n",
    "            f.close()\n",
    "    text = ' '.join(d['postText'])\n",
    "\n",
    "    dic = {'id': d['id'], 'textRank_sumy': text_rank(text) , 'textRank_nltk': nltk_extractive_sumy(text) }\n",
    "\n",
    "    data = data.append(dic, ignore_index = True)\n",
    "\n",
    "    data.to_csv('ext_summarization.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n",
      "miscaptioned\n",
      "false\n",
      "false\n",
      "true\n",
      "false\n",
      "false\n",
      "correct-attribution\n",
      "false\n",
      "true\n",
      "true\n",
      "scam\n",
      "true\n",
      "false\n",
      "true\n",
      "mostly-true\n",
      "true\n",
      "mostly-false\n",
      "originated-as-satire\n",
      "true\n",
      "true\n",
      "true\n",
      "miscaptioned\n",
      "scam\n",
      "scam\n",
      "unproven\n",
      "miscaptioned\n",
      "false\n",
      "true\n",
      "false\n",
      "true\n",
      "false\n",
      "outdated\n",
      "false\n",
      "false\n",
      "mixture\n",
      "correct-attribution\n",
      "true\n",
      "correct-attribution\n",
      "scam\n",
      "miscaptioned\n",
      "miscaptioned\n",
      "legend\n",
      "true\n",
      "true\n",
      "false\n",
      "false\n",
      "outdated\n",
      "unproven\n",
      "misattributed\n",
      "correct-attribution\n",
      "labeled-satire\n",
      "miscaptioned\n",
      "mixture\n",
      "false\n",
      "false\n",
      "correct-attribution\n",
      "false\n",
      "true\n",
      "unproven\n",
      "true\n",
      "mixture\n",
      "true\n",
      "mostly-true\n",
      "true\n",
      "true\n",
      "outdated\n",
      "true\n",
      "false\n",
      "mostly-false\n",
      "true\n",
      "miscaptioned\n",
      "false\n",
      "false\n",
      "false\n",
      "true\n",
      "false\n",
      "mixture\n",
      "unproven\n",
      "scam\n",
      "mostly-false\n",
      "scam\n",
      "mixture\n",
      "miscaptioned\n",
      "false\n",
      "miscaptioned\n",
      "true\n",
      "correct-attribution\n",
      "miscaptioned\n",
      "correct-attribution\n",
      "true\n",
      "true\n",
      "mixture\n",
      "miscaptioned\n",
      "miscaptioned\n",
      "false\n",
      "scam\n",
      "false\n",
      "true\n",
      "scam\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_abs = pd.read_csv('abs_summarization.csv', index_col=0)\n",
    "df_abs.head()\n",
    "\n",
    "df_ext = pd.read_csv('ext_summarization.csv', index_col=0)\n",
    "df_ext.head()\n",
    "\n",
    "\n",
    "for ((i_a, r_a),(i_e, r_e))  in zip(df_abs.iterrows(),df_ext.iterrows()):\n",
    "        if (r_a[0] == r_e[0]): ## tem o mesmo id\n",
    "                df_abs.loc[i_a,'textRank_sumy'] = r_e[1]\n",
    "                df_abs.loc[i_a,'textRank_nltk'] = r_e[2]\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
